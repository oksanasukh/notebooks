{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RVAI SDK - Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq rvai==1.1.0rc51 pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cells\n",
    "\n",
    "For testing purposes we will create a dummy cell that takes an image as input, sleeps for a configurable amount of time to mimmick computation and produce a copy of the image as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# import base classes\n",
    "from dataclasses import dataclass\n",
    "from typing import Type\n",
    "from rvai.base.cell import Cell, cell\n",
    "from rvai.base.data import (\n",
    "    Annotations,\n",
    "    Inputs,\n",
    "    Outputs,\n",
    "    Parameters,\n",
    "    ProcessedParameters,\n",
    ")\n",
    "from rvai.base.context import InferenceContext\n",
    "from rvai.types import Float, Image, Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BenchmarkCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "@dataclass\n",
    "class BenchmarkInputs(Inputs):\n",
    "    image_in: Image = Inputs.field(\n",
    "        name=\"Image\", description=\"An input image.\"\n",
    "    )\n",
    "    counter_in: Integer = Inputs.field(\n",
    "        name=\"Counter\", description=\"Counter input value.\"\n",
    "    )\n",
    "\n",
    "# Outputs\n",
    "@dataclass\n",
    "class BenchmarkOutputs(Outputs):\n",
    "    image_out: Image = Outputs.field(\n",
    "        name=\"Image\", description=\"An input image.\"\n",
    "    )\n",
    "    counter_out: Integer = Outputs.field(\n",
    "        name=\"Counter\", description=\"Counter output value.\"\n",
    "    )\n",
    "\n",
    "# Parameters\n",
    "@dataclass\n",
    "class BenchmarkParameters(Parameters):\n",
    "    delay: Float = Parameters.field(\n",
    "        default=Float(0.1),\n",
    "        name=\"Processing delay\", description=\"Processing delay in seconds\"\n",
    "    )\n",
    "\n",
    "# Cell\n",
    "@cell\n",
    "class BenchmarkCell(Cell):\n",
    "\n",
    "    @classmethod\n",
    "    def call(\n",
    "        cls, context: InferenceContext, parameters: BenchmarkParameters, inputs: BenchmarkInputs,\n",
    "    ) -> BenchmarkOutputs:\n",
    "        # Sleep to mimmick processing\n",
    "        time.sleep(parameters.delay)\n",
    "        outimage = inputs.image_in.copy()\n",
    "        outimage[0, 0, :] = outimage[0, 0, :] / 2\n",
    "        return BenchmarkOutputs(image_out=Image(outimage), counter_out=Integer(int(inputs.counter_in)+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.pipeline import DeclarativePipeline, PipelineCells, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a two types of pipelines with 4 benchmarking cells:\n",
    "- SingleInput: the pipeline input image is connected to each of the 4 cells, counters are chained\n",
    "- Chained: the pipeline input image is connected to the first cell, both image and counter are chained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkingPipelineCells(PipelineCells):\n",
    "    a: BenchmarkCell\n",
    "    b: BenchmarkCell\n",
    "    c: BenchmarkCell\n",
    "    d: BenchmarkCell\n",
    "\n",
    "@pipeline\n",
    "class SingleInputPipeline(DeclarativePipeline):\n",
    "        \n",
    "    cells = BenchmarkingPipelineCells\n",
    "    inputs = {\n",
    "        \"image\": [cells.a.inputs.image_in, cells.b.inputs.image_in, cells.c.inputs.image_in, cells.d.inputs.image_in],\n",
    "        \"counter\": cells.a.inputs.counter_in,\n",
    "    }\n",
    "    outputs = {\"counter\": cells.d.outputs.counter_out}\n",
    "\n",
    "    connections = [\n",
    "        (cells.a.outputs.counter_out, cells.b.inputs.counter_in),\n",
    "        (cells.b.outputs.counter_out, cells.c.inputs.counter_in),\n",
    "        (cells.c.outputs.counter_out, cells.d.inputs.counter_in),\n",
    "    ]\n",
    "\n",
    "@pipeline\n",
    "class ChainedPipeline(DeclarativePipeline):\n",
    "        \n",
    "    cells = BenchmarkingPipelineCells\n",
    "    inputs = {\n",
    "        \"image\": cells.a.inputs.image_in,\n",
    "        \"counter\": cells.a.inputs.counter_in,\n",
    "    }\n",
    "    outputs = {\"counter\": cells.d.outputs.counter_out}\n",
    "\n",
    "    connections = [\n",
    "        (cells.a.outputs.counter_out, cells.b.inputs.counter_in),\n",
    "        (cells.a.outputs.image_out, cells.b.inputs.image_in),\n",
    "        (cells.b.outputs.counter_out, cells.c.inputs.counter_in),\n",
    "        (cells.b.outputs.image_out, cells.c.inputs.image_in),\n",
    "        (cells.c.outputs.counter_out, cells.d.inputs.counter_in),\n",
    "        (cells.c.outputs.image_out, cells.d.inputs.image_in),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleinput_pipeline = SingleInputPipeline.build()\n",
    "# %matplotlib inline\n",
    "singleinput_pipeline.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chained_pipeline = ChainedPipeline.build()\n",
    "# %matplotlib inline\n",
    "chained_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define benchmarking setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sizes = {\n",
    "    \"dummy\": (1, 1, 3),\n",
    "    \"480p\": (480, 852, 3),\n",
    "    \"720p\": (720, 1280, 3),\n",
    "    \"1080p\": (1080, 1920, 3),\n",
    "}\n",
    "pipeline_fps = [(20, 10), (40, 20), (100, 50), (200, 100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "from rvai.base.runtime import Inference\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def benchmark_task(runtime, task, benchmark_images, replicas=None):\n",
    "    # Start an inference process for the task\n",
    "    proc = runtime.start_inference(task)\n",
    "    \n",
    "    # Scale if needed\n",
    "    if replicas is not None:\n",
    "        proc.set_replicas(replicas)\n",
    "    # Do a couple of predictions or warmup\n",
    "    for i in range(5):\n",
    "        out = proc.predict({\"image\": benchmark_images[i], \"counter\": Integer(0)}).result()\n",
    "        # Check that output is valid\n",
    "        assert int(out[\"counter\"]) == 4\n",
    "        \n",
    "    # Do throughput measurements\n",
    "    n_iter = len(benchmark_images)\n",
    "    start = time.time()\n",
    "    # Do prediction requests\n",
    "    for i in range(n_iter):\n",
    "        fut = proc.predict({\"image\": benchmark_images[i], \"counter\": Integer(0)})\n",
    "    # Get the last result\n",
    "    fut.result()\n",
    "    stop = time.time()\n",
    "    \n",
    "    # Estimate throughput\n",
    "    throughput = n_iter/(stop-start)\n",
    "    \n",
    "    # Measure the latency\n",
    "    latency = timeit.timeit(proc.predict({\"image\": benchmark_images[0], \"counter\": Integer(0)}).result, number=n_iter)\n",
    "    \n",
    "    # Stop the process\n",
    "    proc.stop()\n",
    "    \n",
    "    return throughput, latency\n",
    "\n",
    "\n",
    "def benchmark(runtime, pipeline, n_iter, replicas=None):\n",
    "    results = defaultdict(dict)\n",
    "    for img_name, image_size in image_sizes.items():\n",
    "        # Prepare data for testing\n",
    "        images = [Image(np.random.randint(255, size=image_size, dtype=np.uint8)) for _ in range(n_iter)]\n",
    "        results[img_name] = {}\n",
    "        for default_fps, bottleneck_fps in pipeline_fps:\n",
    "            # Construct inference task\n",
    "            inference = Inference(pipeline=pipeline, parameters={\n",
    "                \"a\": BenchmarkParameters(delay=Float(1./default_fps)),\n",
    "                \"b\": BenchmarkParameters(delay=Float(1./bottleneck_fps)),\n",
    "                \"c\": BenchmarkParameters(delay=Float(1./default_fps)),\n",
    "                \"d\": BenchmarkParameters(delay=Float(1./default_fps)),\n",
    "            })\n",
    "            # Calculate theoretical performance\n",
    "            theoretical_latency = (3./default_fps + 1./bottleneck_fps)\n",
    "            sequence_fps = 1./theoretical_latency\n",
    "            \n",
    "            # Perform benchmarking\n",
    "            res = benchmark_task(runtime, inference, images, replicas=replicas)\n",
    "            results[img_name][(default_fps, bottleneck_fps)] = res\n",
    "    return results\n",
    "            \n",
    "\n",
    "def print_results(results):\n",
    "    for img_name, img_results in results.items():\n",
    "        image = images[img_name]\n",
    "        print(f'Results for {img_name} image ({\"x\".join([str(i) for i in image.shape])})')\n",
    "        for (default_fps, bottleneck_fps), (throughput, latency) in img_results.items():\n",
    "            # Calculate theoretical performance\n",
    "            theoretical_latency = (3./default_fps + 1./bottleneck_fps)\n",
    "            sequence_fps = 1./theoretical_latency\n",
    "            \n",
    "            print(f'  * Default cell {default_fps:.2f}FPS - bottleneck {bottleneck_fps:.2f}FPS')\n",
    "            print(f'    - Throughput {throughput:.2f}FPS ({sequence_fps:.2f}FPS - {bottleneck_fps:.2f}FPS)')\n",
    "            print(f'    - Latency {latency:.3f}s ({theoretical_latency:.3f}s)')\n",
    "            \n",
    "            \n",
    "def print_combined(debug_results, ray_results, scaled_results):\n",
    "    plot_theoretical = {img_name: [] for img_name in debug_results}\n",
    "    plot_debug = {img_name: [] for img_name in debug_results}\n",
    "    plot_ray = {img_name: [] for img_name in debug_results}\n",
    "    plot_scaled = {img_name: [] for img_name in debug_results}\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    subplot = 141\n",
    "    for img_name in debug_results:\n",
    "        image_size = image_sizes[img_name]\n",
    "        print(f'Results for {img_name} image ({\"x\".join([str(i) for i in image_size])})')\n",
    "        for default_fps, bottleneck_fps in debug_results[img_name]:\n",
    "            # Calculate theoretical performance\n",
    "            theoretical_latency = (3./default_fps + 1./bottleneck_fps)\n",
    "            sequence_fps = 1./theoretical_latency\n",
    "            \n",
    "            # Get results\n",
    "            debug_throughput, debug_latency = debug_results[img_name][(default_fps, bottleneck_fps)]\n",
    "            ray_throughput, ray_latency = ray_results[img_name][(default_fps, bottleneck_fps)]\n",
    "            scaled_throughput, scaled_latency = scaled_results[img_name][(default_fps, bottleneck_fps)]\n",
    "            \n",
    "            print(f'  * Default cell {default_fps:.2f}FPS - bottleneck {bottleneck_fps:.2f}FPS')\n",
    "            print(f'    - Throughput:')\n",
    "            print(f'      - Theoretical         {bottleneck_fps:03.2f}FPS ({sequence_fps:03.2f}FPS sequential)')\n",
    "            print(f'      - Debug runtime       {debug_throughput:03.2f}FPS')\n",
    "            print(f'      - Ray runtime         {ray_throughput:03.2f}FPS')\n",
    "            print(f'      - Scaled bottleneck   {scaled_throughput:03.2f}FPS')\n",
    "            print(f'    - Latency:')\n",
    "            print(f'      - Theoretical         {theoretical_latency:.3f}s')\n",
    "            print(f'      - Debug runtime       {debug_latency:.3f}s')\n",
    "            print(f'      - Ray runtime         {ray_latency:.3f}s')\n",
    "            print(f'      - Scaled bottleneck   {scaled_latency:.3f}s')        \n",
    "            plot_theoretical[img_name].append(bottleneck_fps)\n",
    "            plot_debug[img_name].append(debug_throughput)\n",
    "            plot_ray[img_name].append(ray_throughput)\n",
    "            plot_scaled[img_name].append(scaled_throughput)\n",
    "        # plotting the points\n",
    "        plt.subplot(subplot)\n",
    "        plt.plot(plot_theoretical[img_name], plot_theoretical[img_name], label=f'Theoretic',linestyle='dotted')\n",
    "        plt.plot(plot_theoretical[img_name], plot_debug[img_name], label=f'Debug')\n",
    "        plt.plot(plot_theoretical[img_name], plot_ray[img_name], label=f'Ray') \n",
    "        plt.plot(plot_theoretical[img_name], plot_scaled[img_name], label=f'Ray Scaled') \n",
    "        subplot += 1\n",
    "\n",
    "        # naming the x axis \n",
    "        plt.xlabel('Theoretical Bottleneck FPS') \n",
    "        # naming the y axis \n",
    "        plt.ylabel('Experimental Pipeline FPS') \n",
    "\n",
    "        # giving a title to my graph \n",
    "        plt.title(f'Runtime Benchmarks FPS - {img_name}') \n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    # function to show the plot \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.runtime import init\n",
    "\n",
    "# Benchmark size\n",
    "n_loops = 10\n",
    "\n",
    "# Choose which pipeline to benchmark\n",
    "chained = True\n",
    "if chained:\n",
    "    pipeline = chained_pipeline\n",
    "else:\n",
    "    pipeline = singleinput_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug runtime benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# init debug runtime\n",
    "debug_rt = init(\"debug\")\n",
    "\n",
    "# Perform benchmarking\n",
    "debug_results = benchmark(debug_rt, pipeline, n_loops)\n",
    "# Stop the runtime\n",
    "debug_rt.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray runtime benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "# init debug runtime\n",
    "ray_rt = init(\"ray\")\n",
    "\n",
    "# Perform benchmarking\n",
    "ray_results = benchmark(ray_rt, pipeline, n_loops)\n",
    "# Stop the runtime\n",
    "ray_rt.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled bottleneck Ray runtime benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# shutdown ray to be sure\n",
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "# init debug runtime\n",
    "ray_rt = init(\"ray\")\n",
    "\n",
    "# scale bottleneck to 2\n",
    "replicas = {'b': 2}  # cell b is the bottleneck\n",
    "\n",
    "# Perform benchmarking\n",
    "scaled_results = benchmark(ray_rt, pipeline, n_loops, replicas=replicas)\n",
    "\n",
    "# Stop the runtime\n",
    "ray_rt.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print benchmarking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_combined(debug_results, ray_results, scaled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
