{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RVAI TensorRT Inference Server Example\n",
    "\n",
    "In this example we will reuse the ImageClassifierCell from the `mnist_training.ipynb` tutorial and expand it with TensorRT Inference Server capabilities.\n",
    "\n",
    "The first part of this tutorial will duplicate quite some steps from the training notebook.\n",
    "\n",
    "**Important note:** This tutorial requires TensorRT Inference Server to be installed on your system. You can use the tutorial docker environment for this. If you want to be able to use your GPU in this environment, you need at least version 440.xx for your hosts NVIDIA drivers. When this is not the case, you will only be able to execute this tutorial on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "First, let's install all the prerequisites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq rvai==1.1.0rc51 pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global notebook configuration\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a TRTISCell\n",
    "Let us create a Cell now. A Cell represents the smalles building block in RVAI. Since our Cell should have TensorRT Inference Server support, we select the `TRTISCell` base class. The basic skeleton of a `TRTISCell` can be found in the [docs [1]](https://base.rvai.dev/rvai.base.trtis.html#rvai.base.trtis.cell.TRTISCell). The `TRTISCell` extends the `TrainableCell` by providing a `convert_to_trtis_model` and `trtis_predict` methods. These methods convert a model (loaded by `load_model`) to a model that's compatible with TensorRT Inference Server or perform a prediction step using a `TRTISClient` respectively.\n",
    "\n",
    "- [1] https://base.rvai.dev/rvai.base.trtis.html#rvai.base.trtis.cell.TRTISCell\n",
    "\n",
    "### Cell IO\n",
    "We can reuse the IO of the `ImageClassificationCell` from the `mnist_training.ipynb` notebook. For details of the code in this section, we refer to that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from rvai.base.data import Inputs, Outputs, Samples, Annotations, Parameters, Metrics\n",
    "from rvai.types import Float, Image, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference mode IO\n",
    "\n",
    "@dataclass\n",
    "class ImageClassificationInputs(Inputs):\n",
    "    image: Image = Inputs.field(\n",
    "        name=\"Image\", description=\"The image to be classified.\")\n",
    "\n",
    "@dataclass\n",
    "class ImageClassificationOutputs(Outputs):\n",
    "    label: Integer = Outputs.field(\n",
    "        name=\"Class\", description=\"The class of the image.\")\n",
    "\n",
    "# Training mode IO\n",
    "        \n",
    "@dataclass\n",
    "class ImageClassificationSamples(Samples, ImageClassificationInputs):\n",
    "    \"\"\"Inherits from ImageClassificationInputs because the Samples this Cell expects during training are the same as its inputs.\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class ImageClassificationAnnotations(Annotations, ImageClassificationOutputs):\n",
    "    \"\"\"Inherits from ImageClassificationOutputs because the Annotations this Cell expects during training are the same as its outputs.\"\"\"\n",
    "    \n",
    "@dataclass\n",
    "class ImageClassificationMetrics(Metrics):\n",
    "    acc: Float = Metrics.field(name=\"Accuracy\", short_name=\"acc\", performance=True)\n",
    "    loss: Float = Metrics.field(name=\"Loss\")\n",
    "    val_acc: Optional[Float] = Metrics.field(\n",
    "        name=\"Validation Accuracy\", default=None\n",
    "    )\n",
    "    val_loss: Optional[Float] = Metrics.field(\n",
    "        name=\"Validation Loss\", default=None\n",
    "    )\n",
    "    \n",
    "# Parameters\n",
    "\n",
    "@dataclass\n",
    "class ImageClassificationParameters(Parameters):\n",
    "    epochs: Integer = Parameters.field(default=Integer(2), name=\"Epochs\", description=\"The amount of times the training loop should process the data.\")\n",
    "    batch_size: Integer = Parameters.field(default=Integer(4), name=\"Batch Size\", description=\"SGD mini-batch size.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell Body\n",
    "Now, let's actually create the Cell! Also this section contains quite some duplication from the `mnist_training.ipynb` notebook. The updated parts will be clearly marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary RVAI imports:\n",
    "from rvai.base.cell import cell # used as a decorator to register a cell in RVAI\n",
    "# ==================\n",
    "# BEGIN TRTIS UPDATE\n",
    "from rvai.base.trtis.cell import TRTISCell # base class, defines main functionality\n",
    "from rvai.base.trtis.model import TRTISModel # wrapper for TRTIS compatible model\n",
    "from rvai.base.trtis.client import TRTISClient # client that allows us to use the deployed models\n",
    "from rvai.base.trtis.utils import keras_to_trtismodel # helper function to convert a keras model\n",
    "# END TRTIS UPDATE\n",
    "# ================\n",
    "\n",
    "\n",
    "# used for typing:\n",
    "from rvai.base.cell import CellMode # enum, defines what mode the cell is running in\n",
    "from rvai.base.data import Example, Dataset, DatasetConfig, Metrics\n",
    "from rvai.base.context import InferenceContext, ModelContext, ParameterContext, TestContext, TrainingContext # required argument for most cell methods\n",
    "from rvai.base.training import Model, ModelConfig, ModelPath\n",
    "from typing import Optional, Tuple, Sequence\n",
    "\n",
    "# used in implementation:\n",
    "# ==================\n",
    "# BEGIN TRTIS UPDATE\n",
    "import cv2  # image manipulation\n",
    "import tempfile  # needed to create temporary conversion folders\n",
    "# END TRTIS UPDATE\n",
    "# ================\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.autograph.set_verbosity(1)\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from rvai.base import compat\n",
    "from rvai.base.training import TrainingSession\n",
    "from rvai.base.test import TestSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cell\n",
    "class ImageClassificationCell(TRTISCell):\n",
    "        \n",
    "    # ==================\n",
    "    # BEGIN TRTIS UPDATE\n",
    "    @classmethod\n",
    "    def load_model(\n",
    "        cls,\n",
    "        context: ModelContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        model_path: Optional[str],\n",
    "        dataset_config: Optional[DatasetConfig],\n",
    "    ) -> Tuple[tf.keras.models.Model, ModelConfig]:\n",
    "        # When we load the model for TRTIS mode, we try to avoid using a GPU\n",
    "        # because the model will only be used for conversion\n",
    "        if context.trtis_mode:\n",
    "            with tf.device('/cpu:0'):\n",
    "                model = cls._do_load_model(model_path=model_path)\n",
    "        else:\n",
    "            model = cls._do_load_model(model_path=model_path)\n",
    "        return model, None\n",
    "    \n",
    "    @classmethod\n",
    "    def _do_load_model(cls, model_path: Optional[str]) -> tf.keras.models.Model:\n",
    "        if model_path is not None:\n",
    "            return tf.keras.models.load_model(model_path)\n",
    "        else:\n",
    "            model = tf.keras.Sequential()\n",
    "            model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "            model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "            model.add(tf.keras.layers.Dropout(0.3))\n",
    "            model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "            model.add(tf.keras.layers.MaxPooling2D(pool_size=2))\n",
    "            model.add(tf.keras.layers.Dropout(0.3))\n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "            model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "            model.add(tf.keras.layers.Dropout(0.5))\n",
    "            model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "            model.compile(loss='categorical_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    # END TRTIS UPDATE\n",
    "    # ================\n",
    "\n",
    "    @classmethod\n",
    "    def _unpack_example(\n",
    "        cls,\n",
    "        example: Example[ImageClassificationSamples, ImageClassificationAnnotations],\n",
    "    ) -> Tuple[np.ndarray, int]:\n",
    "\n",
    "        samples: ImageClassificationSamples = example[0]\n",
    "        annotations: ImageClassificationAnnotations = example[1]\n",
    "\n",
    "        # standardize image input\n",
    "        image = np.atleast_3d(samples.image)\n",
    "        label = int(annotations.label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    @classmethod\n",
    "    def _collate_batch(\n",
    "        cls,\n",
    "        examples: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        x, y = zip(*examples)\n",
    "\n",
    "        images: np.ndarray = np.stack(arrays=x, axis=0)\n",
    "        labels: np.ndarray = tf.keras.utils.to_categorical(\n",
    "            y=y, num_classes=10, dtype=np.float32\n",
    "        )\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    @classmethod\n",
    "    def train(\n",
    "        cls,\n",
    "        context: TrainingContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        model: tf.keras.models.Model,\n",
    "        model_config: ModelConfig,\n",
    "        train_dataset: Dataset[\n",
    "            ImageClassificationSamples, ImageClassificationAnnotations\n",
    "        ],\n",
    "        validation_dataset: Dataset[\n",
    "            ImageClassificationSamples, ImageClassificationAnnotations\n",
    "        ],\n",
    "        dataset_config: Optional[DatasetConfig]\n",
    "    ) -> TrainingSession[ImageClassificationMetrics]:\n",
    "        # Integer -> int\n",
    "        batch_size = int(parameters.batch_size)\n",
    "\n",
    "        train_generator = compat.keras.as_generator(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            process_example=cls._unpack_example,\n",
    "            process_batch=cls._collate_batch,\n",
    "        )\n",
    "\n",
    "        validation_generator = compat.keras.as_generator(\n",
    "            validation_dataset,\n",
    "            batch_size=batch_size,\n",
    "            process_example=cls._unpack_example,\n",
    "            process_batch=cls._collate_batch,\n",
    "        )\n",
    "\n",
    "        nb_epochs = int(parameters.epochs)\n",
    "        nb_training_batches = int(len(train_dataset) // batch_size)\n",
    "        nb_validation_batches = int(len(validation_dataset) // batch_size)\n",
    "\n",
    "        model.fit_generator(\n",
    "            generator=train_generator,\n",
    "            steps_per_epoch=nb_training_batches,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=nb_validation_batches,\n",
    "            epochs=nb_epochs,\n",
    "            verbose=0,\n",
    "            callbacks=[compat.keras.training_update_callback(\n",
    "                context=context,\n",
    "                metrics=ImageClassificationMetrics,\n",
    "            )],\n",
    "        )\n",
    "\n",
    "        model_path = context.get_model_path()\n",
    "\n",
    "        tf.keras.models.save_model(model=model, filepath=model_path)\n",
    "\n",
    "        return model_path\n",
    "\n",
    "    @classmethod\n",
    "    def test(\n",
    "        cls,\n",
    "        context: TestContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        model: tf.keras.models.Model,\n",
    "        model_config: ModelConfig,\n",
    "        test_dataset: Dataset[\n",
    "            ImageClassificationSamples, ImageClassificationAnnotations\n",
    "        ],\n",
    "        dataset_config: Optional[DatasetConfig]\n",
    "    ) -> TestSession[ImageClassificationMetrics]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # ==================\n",
    "    # BEGIN TRTIS UPDATE \n",
    "    @classmethod\n",
    "    def predict(\n",
    "        cls,\n",
    "        context: InferenceContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        model: tf.keras.models.Model,\n",
    "        model_config: ModelConfig,\n",
    "        inputs: ImageClassificationInputs,\n",
    "    ) -> ImageClassificationOutputs:\n",
    "        image = inputs.image\n",
    "        # Get the required input size\n",
    "        input_shape = model.inputs[0].get_shape().as_list()\n",
    "        h, w = input_shape[1], input_shape[2]\n",
    "        # Resize the input image\n",
    "        inp = cv2.resize(image, (w, h))\n",
    "        # Make sure that the input still has 3 dimensions\n",
    "        inp = np.atleast_3d(inp)\n",
    "        # Perform prediction\n",
    "        predictions = model.predict(np.array([inp]))\n",
    "        print(f'Prediction done')\n",
    "        # Get the label\n",
    "        label = predictions[0].argmax()\n",
    "        # Return output\n",
    "        return ImageClassificationOutputs(label=Integer(label))\n",
    "        \n",
    "    @classmethod\n",
    "    def convert_to_trtis_model(\n",
    "        cls,\n",
    "        context: ModelContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        model: tf.keras.models.Model,\n",
    "        model_config: ModelConfig,\n",
    "    ) -> TRTISModel:\n",
    "        # Create a temp directory where to put the converted model\n",
    "        outfolder = tempfile.mkdtemp()\n",
    "        return keras_to_trtismodel(\n",
    "            model=model,  # keras model to convert\n",
    "            model_path=outfolder,  # output folder where to converted model data can be stored\n",
    "            max_batch_size=16  # maximum allowed batch size for the model\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def trtis_predict(\n",
    "        cls,\n",
    "        context: InferenceContext,\n",
    "        parameters: ImageClassificationParameters,\n",
    "        trtis_client: TRTISClient,\n",
    "        model_config: ModelConfig,\n",
    "        inputs: ImageClassificationInputs,\n",
    "    ) -> ImageClassificationOutputs:\n",
    "        image = inputs.image\n",
    "        # Get the input and output layer of the model\n",
    "        input_layer = trtis_client.get_model_spec().input_layers[0]\n",
    "        output_layer = trtis_client.get_model_spec().output_layers[0]\n",
    "        # Get the input shape\n",
    "        h, w = input_layer.dims[0], input_layer.dims[1]  # dims don't have batch dimension\n",
    "        # Resize the input and convert to required data format\n",
    "        inp = cv2.resize(image, (w, h)).astype(input_layer.data_type.to_np())\n",
    "        # Make sure that the input still has 3 dimensions\n",
    "        inp = np.atleast_3d(inp)\n",
    "        # Perform inference. This takes a mapping from layer name to input data\n",
    "        result = trtis_client.infer({input_layer.name: inp})\n",
    "        # Get the output data\n",
    "        predictions = result.get(output_layer.name)\n",
    "        # Get the label\n",
    "        label = predictions.argmax()\n",
    "        # Return output\n",
    "        return ImageClassificationOutputs(label=Integer(label))\n",
    "    # END TRTIS UPDATE\n",
    "    # ================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss.\n",
    "\n",
    "#### `convert_to_trtis_model`\n",
    "TensorRT Inference Server supports several data model types, including Tensorflow savedmodel, Tensorflow frozen graphs, onnx, pytorch, ... Next to the actual model data, the inference server also needs a configuration file specifying some details of the model. The `convert_to_trtis_model` function is used to convert your normal model, loaded via `load_model`, to a model that can be used by the inference server.\n",
    "\n",
    "In the `rvai.base.trtis.utils`, we provide some convenience methods to facilitate this conversion for you, for example for Keras models or Tensorflow frozen graphs. In this tutorial we use the `keras_to_trtismodel` convenience method. This method is given a keras model, a model path where the converted model can be stored and a maximum batch size, and returns the converted model.\n",
    "\n",
    "\n",
    "#### `trtis_predict`\n",
    "The `trtis_predict` method has the same functionality as the `predict` method, but uses a `TRTISClient`, connected to your converted model on a TensorRT Inference Server to do the predictions, rather than your plain model.\n",
    "\n",
    "We use the `infer` method of the `TRTISClient` to perform the inference. This method takes a mapping between input layer names (can be fetched from the `TRTISClient` as seen in this tutorial) and it's input data as an argument. The `infer` call returns a mapping between output layer names and the resulting output data. Similarly, the `infer_batch` method can be used to perform predictions on batches of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Pipeline\n",
    "Also this looks exactly like in the `mnist_training.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.pipeline import DeclarativePipeline, PipelineCells, DeclarativeTrainingPipeline, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCells(PipelineCells):\n",
    "    classifier: ImageClassificationCell\n",
    "\n",
    "@pipeline\n",
    "class ImageClassificationTrainingPipeline(DeclarativeTrainingPipeline):\n",
    "    cells = TrainingCells\n",
    "    train = cells.classifier\n",
    "    samples = [cells.classifier.samples.image]\n",
    "    annotations = [cells.classifier.annotations.label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceCells(PipelineCells):\n",
    "    classifier: ImageClassificationCell\n",
    "\n",
    "@pipeline\n",
    "class ImageClassificationPipeline(DeclarativePipeline):\n",
    "    cells = InferenceCells\n",
    "    inputs = {\"image\": cells.classifier.inputs.image}\n",
    "    outputs = {\"label\": cells.classifier.outputs.label}\n",
    "    training_pipelines = {\n",
    "        cells.classifier: ImageClassificationTrainingPipeline\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline = ImageClassificationPipeline.build()\n",
    "training_pipeline = inference_pipeline.get_training_pipeline(\"classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The training part is exactly the same as in the `mnist_training.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required RVAI base class\n",
    "from rvai.base.data import Dataset\n",
    "\n",
    "# used for typing\n",
    "from rvai.types import Image, Integer\n",
    "from typing import Sequence, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# actual data\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# some imports for displaying data\n",
    "from IPython.display import display, HTML\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(\n",
    "    Dataset[ImageClassificationSamples, ImageClassificationAnnotations]\n",
    "):\n",
    "    def __init__(\n",
    "        self, images: Sequence[np.ndarray], labels: Sequence[np.ndarray]\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(\n",
    "        self, index\n",
    "    ) -> Tuple[ImageClassificationSamples, ImageClassificationAnnotations]:\n",
    "        return (\n",
    "            ImageClassificationSamples(image=Image(self.images[index])),\n",
    "            ImageClassificationAnnotations(label=Integer(self.labels[index])),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "# Class names for FashionMNIST\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "train_dataset, validation_dataset = (FashionMNISTDataset(images=images, labels=labels) for images, labels in fashion_mnist.load_data())\n",
    "\n",
    "# display an example image and its label\n",
    "samples, annotations = train_dataset[0]\n",
    "display(PIL.Image.fromarray(samples.image)); print(class_names[annotations.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.runtime import init, Training, Inference\n",
    "from rvai.base.training import Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a runtime, we choose the debug runtime\n",
    "runtime = init(\"debug\")\n",
    "\n",
    "# generate a training pipeline\n",
    "training_pipeline = inference_pipeline.get_training_pipeline(\"classifier\")\n",
    "\n",
    "# configure a training task\n",
    "training = Training(\n",
    "    pipeline=training_pipeline,\n",
    "    models={}, # no previous models yet\n",
    "    parameters={\"classifier\": ImageClassificationParameters()}, # defaults are fine for us \n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "training_loop = runtime.start_training(training)\n",
    "\n",
    "print('Starting training')\n",
    "for update in training_loop.updates():\n",
    "    print(f\"\\r[{update.progress * 100:.3}%] - accuracy: {update.metrics.acc}\", end='')    \n",
    "trained_model_path = training_loop.result()\n",
    "print('\\nTraining done. Model can be found at:', trained_model_path)\n",
    "# Stop the training process\n",
    "training_loop.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Now that we have a trained model, we can start doing inference tasks.\n",
    "\n",
    "### Normal Inference (no TensorRT Inference Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create inference task\n",
    "inference = Inference(\n",
    "    pipeline=inference_pipeline, \n",
    "    models={\"classifier\": trained_model_path},  # use the trained model\n",
    "    parameters={\"classifier\": ImageClassificationParameters()},  # defaults are fine for us \n",
    ")\n",
    "proc = runtime.start_inference(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample from the validation dataset. Run this cell multiple times to get different samples\n",
    "idx = np.random.randint(len(validation_dataset))\n",
    "sample, annotation = validation_dataset[idx]\n",
    "# Perform inference and get the result\n",
    "pred = proc.predict({\"image\": sample.image})\n",
    "result = pred.result()\n",
    "\n",
    "# Display the image and the label vs prediction\n",
    "display(PIL.Image.fromarray(sample.image))\n",
    "print(f'Label: {class_names[annotation.label]}, prediction: {class_names[result.get(\"label\")]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the inference process\n",
    "proc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Inference Server Mode\n",
    "This works exactly the same as normal inference tasks, with the exception that the `Inference` task is started with the `trtis_enabled` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.trtis.resources import TRTISResources\n",
    "\n",
    "# create inference task, similar as before, but enable TRTIS by providing an TRTISResources object with required cpus and gpus\n",
    "inference = Inference(\n",
    "    pipeline=inference_pipeline, \n",
    "    models={\"classifier\": trained_model_path},  # use the trained model\n",
    "    parameters={\"classifier\": ImageClassificationParameters()},  # defaults are fine for us \n",
    "    trtis_resources=TRTISResources(gpus=1.0),  # enable TensorRT Inference Server mode\n",
    ")\n",
    "proc = runtime.start_inference(inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random sample from the validation dataset. Run this cell multiple times to get different samples\n",
    "idx = np.random.randint(len(validation_dataset))\n",
    "sample, annotation = validation_dataset[idx]\n",
    "# Perform inference and get the result\n",
    "pred = proc.predict({\"image\": sample.image})\n",
    "result = pred.result()\n",
    "\n",
    "# Display the image and the label vs prediction\n",
    "display(PIL.Image.fromarray(sample.image))\n",
    "print(f'Label: {class_names[annotation.label]}, prediction: {class_names[result.get(\"label\")]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the inference process and runtime\n",
    "proc.stop()\n",
    "runtime.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
