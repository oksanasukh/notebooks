{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline drivers\n",
    "\n",
    "After starting an `InferenceProcess` on a `Runtime` it is possible to manually run predictions via the `InferenceProcess.predict` method where your inputs are send to the pipeline's \"virtual source\" and the results are fetched from the pipeline's \"virtual sink\". While this good for initial testing, at some point the pipeline will need to be used in production where we want to connect the pipeline to some stream or HTTP server and let the predictions run automatically.\n",
    "\n",
    "This is where **Pipeline drivers** come into play.\n",
    "\n",
    "A `PipelineDriver` is a wrapper of the `InferenceProcess`.\n",
    "The goal of the driver is to *drive* the pipeline with input generators (sources) and output processors (sinks).\n",
    "Some examples:\n",
    "* an HTTPServer that handles prediction requests and sends the result back as a response\n",
    "* a driver that fetches images from a stream and writes the results to a database\n",
    "\n",
    "![title](img/PipelineDriver.png)\n",
    "\n",
    "## Reasoning behind pipeline drivers\n",
    "\n",
    "* The reason why a 'source' and 'sink' is not part of the pipeline is to make it more easy to link a pipeline with a different stream/http server\n",
    "without having to make dupplications or variations of the same pipeline. Instead a pipeline has a virtual source and sink that can be linked with any driver to fit a specific use case.\n",
    "* Multiple drivers can be added for the same `InferenceProcess` process while using the same GPU resources. The SDK ensures that the `PipelineState` between all these streams are not intertwined as long as the driver implements sets the correct *state ids* for each prediction.\n",
    "* Another advantage is that you don't need to manually startup these processes yourself since it's the runtime's task to start and stop the drivers.\n",
    "\n",
    "## PipelineDriver interface\n",
    "\n",
    "The separation between a source / sink is not always clear and is sometimes tightly coupled. For example for an HTTP server the HTTPRequest might\n",
    "represent the source and the HTTPResponse might represent the sink. Therefore a simple `PipelineDriver` base class is provided that has a run method that receives an `InferenceProcess` that leaves the source/sink design to the implementer:\n",
    "\n",
    "```python\n",
    "# pseudo implementation\n",
    "class PipelineDriver(metaclass=ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def run(self, inference: \"InferenceProcess\"):\n",
    "        \"\"\"Start the driver.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the driver and cleanup resources.\"\"\"\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "The SDK already provides a few drivers out of the box:\n",
    "* `FastAPIDriver` that provides an HTTP Rest API server for the pipeline\n",
    "* `SourceSinkDriver` that that can fetch inputs from arbitrary `Source`s (e.g. RTSPSource) and send the results to arbitrary `Sink`s (e.g. DBWriterSink)\n",
    "\n",
    "But first we will create a minimal dummy pipeline to demonstrate the drivers concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qqq rvai==1.1.0rc51 pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from rvai.base.cell import Cell, cell\n",
    "from rvai.base.data import (\n",
    "    Inputs,\n",
    "    Outputs,\n",
    "    Parameters,\n",
    "    State,\n",
    ")\n",
    "from rvai.base.context import InferenceContext\n",
    "from rvai.base.pipeline import PipelineFactory\n",
    "from rvai.types import Integer\n",
    "\n",
    "# Cell\n",
    "@dataclass\n",
    "class MyInputs(Inputs):\n",
    "    integer: Integer = Inputs.field(name=\"Dummy input\", description=\"Some dummy input\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyOutputs(Outputs):\n",
    "    integer: Integer = Outputs.field(name=\"Dummy output\", description=\"Some dummy output\")\n",
    "    counter: Integer = Outputs.field(name=\"Counter\", description=\"Prediction counter\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyParameters(Parameters):\n",
    "    adder: Integer = Parameters.field(default=Integer(1), name=\"Adder\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MyState(State):\n",
    "    counter: Integer = State.field(default=Integer(0))\n",
    "\n",
    "\n",
    "@cell(state=MyState)\n",
    "class MyCell(Cell):\n",
    "\n",
    "    @classmethod\n",
    "    def call(\n",
    "        cls, context: InferenceContext, parameters: MyParameters, inputs: MyInputs,\n",
    "    ) -> MyOutputs:\n",
    "        state: MyState = context.state\n",
    "        state.counter = Integer(state.counter + 1)\n",
    "        return MyOutputs(\n",
    "            integer=Integer(inputs.integer + parameters.adder),\n",
    "            counter=state.counter,\n",
    "        )\n",
    "\n",
    "# Pipeline\n",
    "pf = PipelineFactory(name=\"MyPipeline\")\n",
    "my_cell = MyCell()\n",
    "pf.add_cell(ref=\"my-cell\", cell=my_cell)\n",
    "pf.declare_input(ref=\"input\", input=my_cell.inputs.integer)\n",
    "pf.declare_output(ref=\"output\", output=my_cell.outputs.integer)\n",
    "pf.declare_output(ref=\"counter\", output=my_cell.outputs.counter)\n",
    "\n",
    "pipeline = pf.build()\n",
    "pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the pipeline's graph there's a \"virtual source\" and a \"virtual sink\" added. Now we will define a driver that will connect these to an actual source and sink.\n",
    "\n",
    "\n",
    "Now start an inference process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.runtime import Inference, init\n",
    "\n",
    "rt = init(\"debug\")\n",
    "inference = Inference(pipeline=pipeline)\n",
    "\n",
    "proc = rt.start_inference(inference=inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastAPIDriver\n",
    "\n",
    "Adding a `FastAPIDriver` to an inference process is quite easy. The method `InferenceProcess.add_driver` expects a driver reference, a callable that instantiates a driver and optional arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.extensions.drivers.fast_api_driver import FastAPIDriver\n",
    "\n",
    "fast_api_driver_proc = proc.add_driver(\"webapi\", FastAPIDriver)\n",
    "assert fast_api_driver_proc == proc.get_driver(\"webapi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method also returns a handle to the driver process that can be used to manage the spawned driver.\n",
    "\n",
    "The driver process can also be fetched via `proc.get_driver(\"webapi\")` or `proc.list_drivers()` to get an overview of all added drivers.\n",
    "\n",
    "#### Driver services\n",
    "\n",
    "Some drivers like the `FastAPIDriver` can expose a service that an external client can connect to remotely. Those services can be obtained via `PipelineDriverProcess.list_services()` and `PipelineDriverProcess.get_service()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get accesspoints of this driver\n",
    "print(\"FastAPIDriver services:\", \", \".join([s[\"name\"] for s in fast_api_driver_proc.list_services()]))\n",
    "\n",
    "http_service = fast_api_driver_proc.get_service(\"http\")\n",
    "host = http_service[\"taggedAddresses\"][\"wan\"][\"address\"]\n",
    "port = http_service[\"taggedAddresses\"][\"wan\"][\"port\"]\n",
    "fast_api_url = f\"http://{host}:{port}\"\n",
    "print(f\"The API server should be available on {fast_api_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(fast_api_url, width=1000, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SDK also provides a python client. Instead of providing the host and port manually, it is also possible to provide a `PipelineDriverProcess`. The client will then automatically fetch the correct host and por"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.extensions.drivers.fast_api_driver import FastAPIClient, FastAPIClientError\n",
    "\n",
    "client = FastAPIClient(process=fast_api_driver_proc)\n",
    "\n",
    "# wait for api server te be online\n",
    "client.wait()\n",
    "\n",
    "# regular predict\n",
    "print(\"HTTP predict result:\", client.predict({\"input\": Integer(1)}))\n",
    "\n",
    "# prediction task\n",
    "task_id = client.predict_task({\"input\": Integer(1)})\n",
    "print(\"HTTP prediction task:\", task_id)\n",
    "print(\"HTTP prediction task result:\", client.task_result(task_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drivers can be added and removed at runtime. `InferenceProcess.add_driver` returns a `PipelineDriverProcess` which can be stopped via `PipelineDriverProcess.stop` or by giving its `pid` to and `InferenceProcess.remove_driver(driver_proc.pid)`. All running driver processes can be fetched via `InferenceProcess.list_drivers()`. While doing this the actual pipeline and all its resources are still available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rvai.base.exc import DriverNotFoundError\n",
    "from rvai.types import Integer\n",
    "\n",
    "try:\n",
    "    # stop the driver\n",
    "    proc.remove_driver(fast_api_driver_proc.pid)\n",
    "except DriverNotFoundError as e:\n",
    "    print('Could not remove driver:', e)\n",
    "    \n",
    "print('* manual inference still possible:', proc.predict({\"input\": Integer(1)}).result())\n",
    "\n",
    "print(\"* client not responding (as expected):\")\n",
    "try:\n",
    "    client.predict({\"input\": Integer(1)})\n",
    "except FastAPIClientError as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    client.wait(timeout=1)\n",
    "except FastAPIClientError as e:\n",
    "    print(\"Can also be tested via `client.wait()`:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_api_driver_proc = proc.add_driver(\"webapi\", FastAPIDriver)\n",
    "client = FastAPIClient(process=fast_api_driver_proc)\n",
    "client.wait()\n",
    "print(\"Server back online!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver parameters\n",
    "\n",
    "Driver parameters can be accessed and changed via `get_parameters` and `set_parameters` methods on the driver process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_api_params = fast_api_driver_proc.get_parameters()\n",
    "print(f\"Current FastAPI parameters: {fast_api_params}\")\n",
    "fast_api_params.task_ttl = Integer(10)\n",
    "fast_api_driver_proc.set_parameters(fast_api_params)\n",
    "print(f\"Updated FastAPI parameters: {fast_api_driver_proc.get_parameters()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SourceSinkDriver\n",
    "\n",
    "The `SourceSinkDriver` is capable of connecting multiple arbitrary sources and sinks tot the virtual sources and sinks.\n",
    "\n",
    "In order to be able to re-use or add more sources and sinks the interface is standardized via some base class `Source` and `Sink`. The API of those classes can be found in the docs: https://base.rvai.dev/rvai.base.drivers.html\n",
    "\n",
    "For testing purposes the SDK already provides a `DummySource` and `DummySink`. The `DummySource` can be configured to emit any input format at any rate, while the `DummySink` takes a `drain` method (by default it will write the outputs to logging).\n",
    "\n",
    "**!!WARNING** *Drivers may only be instantiated by the runtime. Since the `SourceSinkDriver` expects `Source` and `Sink` instances to be added, we wrap our driver definition in a function that returns a driver instance. Which is ok since the `add_driver` method expects any `Callable` that generates a `PipelineDriver` instance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from rvai.extensions.drivers.source_sink_driver import SourceSinkDriver\n",
    "from rvai.extensions.sources.dummy_source import DummySource\n",
    "from rvai.extensions.sinks.dummy_sink import DummySink\n",
    "from rvai.types import FloatRange\n",
    "\n",
    "# create driver\n",
    "def create_source_sink_driver():\n",
    "    driver = SourceSinkDriver()\n",
    "    \n",
    "    dummy_source = DummySource(format={\"input\": Integer}, params=DummySource.parameters(rate=FloatRange(10)))\n",
    "    dummy_sink = DummySink(drain=lambda x: print(\"\\rdraining \", x, end=\"\"))\n",
    "    \n",
    "    driver.add_source(ref=\"source\", source=dummy_source)\n",
    "    driver.add_sink(ref=\"sink\", sink=dummy_sink)\n",
    "    return driver\n",
    "\n",
    "# add to inference process\n",
    "driver_proc = proc.add_driver(\"dummy-stream\", create_source_sink_driver)\n",
    "print(f\"SourceSinkDriver parameters: {driver_proc.get_parameters()}\")\n",
    "print(f\"SourceSinkDriver services:\", \", \".join([s[\"name\"] for s in driver_proc.list_services()]))\n",
    "time.sleep(5)\n",
    "print(\"\")\n",
    "print(\"Stopping source sink driver\")\n",
    "proc.remove_driver(driver_proc.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom drivers\n",
    "\n",
    "While `SourceSinkDriver` can cover many usecases because of the arbitrary `Source`s and `Sink`s, you sometimes want to be able to write a custom driver.\n",
    "\n",
    "This can be done by subclassing the abstract class`PipelineDriver`. There are a few aspects to creating a driver:\n",
    "\n",
    "#### 1. Defining parameters\n",
    "\n",
    "A driver must define the parameters for the driver itself. This is done be creating a `DriverParameters` dataclass and implementing the `PipelineDriver.get_parameters` and `PipelineDriver.set_parameters` methods\n",
    "\n",
    "#### 2. Requesting resources\n",
    "\n",
    "Optionally, a driver can request some resources by overriding the `PipelineDriver.resources` method and returning a `ResourcesRequest` instance. Currently it's only possible to request ports.\n",
    "\n",
    "#### 3. Implementing the `run` and `stop` methods\n",
    "\n",
    "The 'loop' of the driver needs to be implemented in the `PipelineDriver.run` method. This method receives an `InferenceProcess` which can be used to perform requests on the pipeline and a `Resources` instance which contains the assigned resources that have been requested.\n",
    "\n",
    "Finally the `stop` method must stop the driver loop that has been started in the `run` method.\n",
    "\n",
    "\n",
    "Let's create one that does a certain amount of predictions and prints the average execution time and add it to the inference process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from uuid import uuid4\n",
    "from typing import Optional\n",
    "from rvai.base.drivers import DriverParameters, PipelineDriver\n",
    "from rvai.base.resources import DriverResources, DriverResourcesRequest\n",
    "from rvai.base.runtime import RestrictedInferenceProcess\n",
    "from rvai.types import Integer\n",
    "\n",
    "@dataclass\n",
    "class CustomDriverParameters(DriverParameters):\n",
    "    param: Integer = field(default=Integer(0))\n",
    "\n",
    "class CustomDriver(PipelineDriver):\n",
    "    \"\"\"\n",
    "        Stress test the inference process and print the average execution time.\n",
    "    \"\"\"\n",
    "    def __init__(self, predictions: int = 10, params: Optional[CustomDriverParameters] = None):\n",
    "        self._predictions = predictions\n",
    "        self._params = params or CustomDriverParameters()\n",
    "        self._running = False\n",
    "    \n",
    "    def resources_request(self) -> DriverResourcesRequest:  \n",
    "        # Optionally request resources\n",
    "        return DriverResourcesRequest(ports=[\"dummy-port\"])    \n",
    "        \n",
    "    def run(self, inference: RestrictedInferenceProcess, resources: DriverResources):\n",
    "        print(\"Starting custom driver\")\n",
    "        print(\"Assigned ports:\", resources.ports)\n",
    "        self._running = True\n",
    "        \n",
    "        # create new state id\n",
    "        state_id = str(uuid4())   \n",
    "        \n",
    "        times = []\n",
    "        results = []\n",
    "        for _ in range(self._predictions):\n",
    "            if not self._running:\n",
    "                # respect the `stop` action and stop the loop\n",
    "                print(\"Prematurely stopping test!\")\n",
    "                break\n",
    "            start = time.time()\n",
    "            result = inference.predict(inputs={\"input\": Integer.fake()}, sid=state_id).result()\n",
    "            times.append(time.time() - start)\n",
    "            results.append(result)\n",
    "        \n",
    "        if len(results) >= 2:\n",
    "            print(f\"First result: {results[0]}\")\n",
    "            print(f\"Second result: {results[1]}\")\n",
    "        print(f\"Average time over {len(times)} predictions: {np.round(np.mean(times) * 1000, 1)}ms\")    \n",
    "    \n",
    "    def stop(self):\n",
    "        self._running = False\n",
    "        \n",
    "    def get_parameters(self) -> CustomDriverParameters:\n",
    "        \"\"\"Get parameters.\"\"\"\n",
    "        return self._params\n",
    "\n",
    "    def set_parameters(self, parameters: CustomDriverParameters):\n",
    "        \"\"\"Set parameters.\"\"\"\n",
    "        self._params = parameters\n",
    "\n",
    "driver_proc = proc.add_driver(\"custom\", CustomDriver, predictions=1000)\n",
    "# test premature stop\n",
    "# time.sleep(0.1)\n",
    "# proc.remove_driver(driver_proc.pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop inference process including all drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop the runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging drivers, sources and sinks\n",
    "\n",
    "Packaging of custom drivers, sources and sinks will be similar to how you register cells and pipelines.\n",
    "**WORK IN PROGRESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
